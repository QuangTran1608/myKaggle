{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8985442,"sourceType":"datasetVersion","datasetId":5411381},{"sourceId":8993687,"sourceType":"datasetVersion","datasetId":5417195},{"sourceId":80361,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":67534,"modelId":92608}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\nimport math\nfrom torch import nn\nimport torch.nn.functional as F\n\ndef get_device():\n    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\ndevice = get_device()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:40:37.376809Z","iopub.execute_input":"2024-07-21T17:40:37.377282Z","iopub.status.idle":"2024-07-21T17:40:41.879061Z","shell.execute_reply.started":"2024-07-21T17:40:37.377244Z","shell.execute_reply":"2024-07-21T17:40:41.877619Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/phomt-donotpublic/PhoMT/tokenization/train/train.en\", 'r') as f:\n    eng_sentences = f.read().splitlines()\n    \nwith open(\"/kaggle/input/phomt-donotpublic/PhoMT/tokenization/train/train.vi\", 'r') as f:\n    vie_sentences = f.read().splitlines()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-21T03:58:49.974349Z","iopub.execute_input":"2024-07-21T03:58:49.974942Z","iopub.status.idle":"2024-07-21T03:59:06.953239Z","shell.execute_reply.started":"2024-07-21T03:58:49.974899Z","shell.execute_reply":"2024-07-21T03:59:06.952124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(vie_sentences))\nprint(len(eng_sentences))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:06.954617Z","iopub.execute_input":"2024-07-21T03:59:06.954993Z","iopub.status.idle":"2024-07-21T03:59:06.962181Z","shell.execute_reply.started":"2024-07-21T03:59:06.954962Z","shell.execute_reply":"2024-07-21T03:59:06.960867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will limit the exposure of data here as per my agreement I accept when using this data","metadata":{}},{"cell_type":"code","source":"# Trying to find mismatch in indexes since lengths are different\n# index = len(eng_sentences)//100000*93000\n# print(index)\n# print(eng_sentences[index])\n# print(\"-------------------------\")\n# print(vie_sentences[index])\n\n# Seem like the mismatch happened between these 2 indexes, \n# Just use 92% of data works for me i guess, better than checking for mismatch\n# index = len(eng_sentences)//100000*92000\n# print(index)\n# print(eng_sentences[index])\n# print(\"-------------------------\")\n# print(vie_sentences[index])","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:06.965087Z","iopub.execute_input":"2024-07-21T03:59:06.965505Z","iopub.status.idle":"2024-07-21T03:59:06.975368Z","shell.execute_reply.started":"2024-07-21T03:59:06.965449Z","shell.execute_reply":"2024-07-21T03:59:06.974040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only 92% of the first data is (assumed) to be not mislabeled","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, AutoTokenizer\n\neng_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nvie_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:41:56.055657Z","iopub.execute_input":"2024-07-21T17:41:56.056184Z","iopub.status.idle":"2024-07-21T17:42:00.814994Z","shell.execute_reply.started":"2024-07-21T17:41:56.056147Z","shell.execute_reply":"2024-07-21T17:42:00.813647Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b86aa1b68054d12a153825a7570eaf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510b1b3d7cc34c89b6e644c9169282f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b78f870dbc4342e6891566827e79e490"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7098716f1321490b90eebd45812a5e84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f69f94a32a74165bd25c762f2416ac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cc3f8a39d3041e7bed62b85b53b2dfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8063a14ad1446fdaae815139c92b09f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb8ef5d35f04e3fa424f3703f7dad9a"}},"metadata":{}}]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport pandas as pd\nimport re\nimport string\nimport unicodedata\n\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n      if unicodedata.category(c) != 'Mn')\n\ndef clean_eng_text(text):\n    text = unicode_to_ascii(text.lower().strip())\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"\\r\", \"\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"that is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"'bout\", \"about\", text)\n    text = re.sub(r\"'til\", \"until\", text)\n    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n    text = text.translate(str.maketrans('', '', string.punctuation)) \n    text = re.sub(\"(\\\\W)\",\" \",text) \n    text = re.sub('\\S*\\d\\S*\\s*','', text)\n    return text\n\ndef clean_vie_text(text):\n    text = text.lower().strip()\n    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = re.sub('\\S*\\d\\S*\\s*','', text)\n    return text\n\n# Back up the processed data to save sometime in next run\nif os.path.exists(\"/kaggle/input/phomt-processed/processed.parquet\"):\n    print(\"loading processed file\")\n    df = pd.read_parquet(\"/kaggle/input/phomt-processed/processed.parquet\")\n    eng_tokens = df['eng_tokens']\n    vie_tokens = df['vie_tokens']\n    print('done')\n    \nelse:\n    print(\"Tokenizing file\")\n    print(\"Processing english file\")\n    eng_sentences = [clean_eng_text(sentence) for sentence in eng_sentences]\n    eng_tokens = [eng_tokenizer.encode(sentence) for sentence in eng_sentences]\n    eng_tokens = eng_tokens[:index+1]\n    \n    print(\"Processing vietnamese file\")\n    vie_sentences = [clean_vie_text(sentence) for sentence in vie_sentences]\n    vie_tokens = [vie_tokenizer.encode(sentence) for sentence in vie_sentences]\n    vie_tokens = vie_tokens[:index+1]\n    df = {'eng_tokens': eng_tokens, 'vie_tokens': vie_tokens} \n    df = pd.DataFrame(df)\n    eng_tokens = df['eng_tokens']\n    vie_tokens = df['vie_tokens']\n    df.to_parquet(\"processed.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:42:26.796364Z","iopub.execute_input":"2024-07-21T17:42:26.796797Z","iopub.status.idle":"2024-07-21T17:42:34.427672Z","shell.execute_reply.started":"2024-07-21T17:42:26.796762Z","shell.execute_reply":"2024-07-21T17:42:34.426204Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"loading processed file\ndone\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use only first 80000 rows of data\neng_tokens = eng_tokens[:80000]\nvie_tokens = vie_tokens[:80000]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:42:34.430054Z","iopub.execute_input":"2024-07-21T17:42:34.430602Z","iopub.status.idle":"2024-07-21T17:42:34.436731Z","shell.execute_reply.started":"2024-07-21T17:42:34.430568Z","shell.execute_reply":"2024-07-21T17:42:34.435305Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len_eng_tokens = [len(tokens) for tokens in tqdm(eng_tokens)]\nlen_vie_tokens = [len(tokens) for tokens in tqdm(vie_tokens)]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:42:34.438635Z","iopub.execute_input":"2024-07-21T17:42:34.439561Z","iopub.status.idle":"2024-07-21T17:42:34.615580Z","shell.execute_reply.started":"2024-07-21T17:42:34.439513Z","shell.execute_reply":"2024-07-21T17:42:34.614365Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 80000/80000 [00:00<00:00, 1050687.22it/s]\n100%|██████████| 80000/80000 [00:00<00:00, 998735.95it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(eng_tokenizer.decode(eng_tokens[0]))\nprint(vie_tokenizer.decode(vie_tokens[0]))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:17.645783Z","iopub.execute_input":"2024-07-21T03:59:17.646142Z","iopub.status.idle":"2024-07-21T03:59:17.652447Z","shell.execute_reply.started":"2024-07-21T03:59:17.646111Z","shell.execute_reply":"2024-07-21T03:59:17.651287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ncontext_length_eng = np.percentile(len_eng_tokens, 99)\ncontext_length_vie = np.percentile(len_vie_tokens, 99)\nprint(\"99% of english sentences are under \", context_length_eng, \" tokens\")\nprint(\"99% of vietnamese sentences are under \", context_length_vie, \" tokens\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:42:38.934788Z","iopub.execute_input":"2024-07-21T17:42:38.935241Z","iopub.status.idle":"2024-07-21T17:42:38.967976Z","shell.execute_reply.started":"2024-07-21T17:42:38.935207Z","shell.execute_reply":"2024-07-21T17:42:38.965968Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"99% of english sentences are under  62.0  tokens\n99% of vietnamese sentences are under  75.0  tokens\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use the data within context length\nindexes = (len_eng_tokens <= context_length_eng) & (len_vie_tokens <= context_length_vie)\n\neng_tokens = eng_tokens[indexes] # Implicitly convert to numpy\nvie_tokens = vie_tokens[indexes]","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:53:02.088527Z","iopub.execute_input":"2024-07-21T17:53:02.088949Z","iopub.status.idle":"2024-07-21T17:53:02.124763Z","shell.execute_reply.started":"2024-07-21T17:53:02.088905Z","shell.execute_reply":"2024-07-21T17:53:02.123358Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Weight of losses for classes/tokens to counter the frequency of tokens**","metadata":{}},{"cell_type":"code","source":"# flattened_values = np.concatenate(vie_tokens.to_list())\n# value_counts = pd.Series(flattened_values).value_counts()\n\n# weights = 1.0 / np.log(value_counts + 1)\n# weights = weights / weights.mean()\n\n# num_classes = len(vie_tokenizer.get_vocab().keys())\n# class_weights = torch.zeros(num_classes)\n\n# for i in range(num_classes):\n#     class_weights[i] = weights[i] if i in weights else 1\n\n# class_weights = class_weights.to(device)\n    \n# print(\"Frequency of each label:\")\n# print(value_counts)\n# print(\"\\nWeights for each label (inverse of frequency):\")\n# print(weights)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:17.722550Z","iopub.execute_input":"2024-07-21T03:59:17.723038Z","iopub.status.idle":"2024-07-21T03:59:17.729664Z","shell.execute_reply.started":"2024-07-21T03:59:17.722995Z","shell.execute_reply":"2024-07-21T03:59:17.728388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_eng_tokens = np.array(len_eng_tokens)[indexes]\nlen_vie_tokens = np.array(len_vie_tokens)[indexes]\n\ncontext_length_eng = max(len_eng_tokens)\ncontext_length_vie = max(len_vie_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:53:04.552723Z","iopub.execute_input":"2024-07-21T17:53:04.553149Z","iopub.status.idle":"2024-07-21T17:53:04.603226Z","shell.execute_reply.started":"2024-07-21T17:53:04.553116Z","shell.execute_reply":"2024-07-21T17:53:04.601752Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.hist(len_eng_tokens, bins=range(min(len_eng_tokens), context_length_eng + 1, 1), \n              alpha=0.4, color=\"red\")\nplt.hist(len_vie_tokens, bins=range(min(len_vie_tokens), context_length_vie + 1, 1),\n              alpha=0.4, color=\"blue\")\nlabels = ['English',\"Vietnamese\"]\nplt.legend(labels)\nplt.xlabel(\"length of sentence\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:17.787448Z","iopub.execute_input":"2024-07-21T03:59:17.787909Z","iopub.status.idle":"2024-07-21T03:59:18.855410Z","shell.execute_reply.started":"2024-07-21T03:59:17.787869Z","shell.execute_reply":"2024-07-21T03:59:18.854151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextDataset(Dataset):\n\n    def __init__(self, eng_tokens, vie_tokens):\n        self.eng_tokens = eng_tokens\n        self.vie_tokens = vie_tokens\n\n    def __len__(self):\n        return len(self.eng_tokens)\n\n    def __getitem__(self, idx):\n        return self.eng_tokens[idx], self.vie_tokens[idx]\n\ndataset = TextDataset(eng_tokens.to_list(), vie_tokens.to_list())","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:18.856892Z","iopub.execute_input":"2024-07-21T03:59:18.857271Z","iopub.status.idle":"2024-07-21T03:59:18.868360Z","shell.execute_reply.started":"2024-07-21T03:59:18.857240Z","shell.execute_reply":"2024-07-21T03:59:18.867021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset, batch_size=32, collate_fn=lambda x: x, shuffle=True)\niterator = iter(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:18.869986Z","iopub.execute_input":"2024-07-21T03:59:18.870408Z","iopub.status.idle":"2024-07-21T03:59:18.916553Z","shell.execute_reply.started":"2024-07-21T03:59:18.870369Z","shell.execute_reply":"2024-07-21T03:59:18.915292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch_num, batch in enumerate(iterator):\n    eng_batch, vie_batch = zip(*[batch[i] for i in range(len(batch))])\n    print(eng_batch[0])\n    print(\"DONE\")\n    print(vie_batch[0])\n    break","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:18.918190Z","iopub.execute_input":"2024-07-21T03:59:18.918577Z","iopub.status.idle":"2024-07-21T03:59:18.958473Z","shell.execute_reply.started":"2024-07-21T03:59:18.918546Z","shell.execute_reply":"2024-07-21T03:59:18.957216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transformer architecture**","metadata":{}},{"cell_type":"code","source":"NEG_INFTY = -1e9\n\ndef scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n    if mask is not None:\n        scaled = scaled.permute(1, 0, 2, 3).masked_fill(mask==0, NEG_INFTY)\n        scaled = scaled.permute(1, 0, 2, 3)\n    attention = F.softmax(scaled, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention\n\ndef create_masks(eng_batch, vie_batch):\n    num_sentences = len(eng_batch)\n    look_ahead_mask = torch.ones([context_length_vie, context_length_vie])\n    look_ahead_mask = torch.tril(look_ahead_mask)\n    encoder_self_attention_mask = torch.ones([num_sentences, context_length_eng, context_length_eng])\n    decoder_padding_mask_self_attention = torch.ones([num_sentences, context_length_vie, context_length_vie])\n    decoder_cross_attention_mask = torch.ones([num_sentences, context_length_vie, context_length_eng])\n    \n    for idx in range(num_sentences):\n        eng_length = len(eng_batch[idx])\n        vie_length = len(vie_batch[idx])\n        encoder_self_attention_mask[idx, :, eng_length:] = 0\n        encoder_self_attention_mask[idx, eng_length:, :] = 0\n        decoder_padding_mask_self_attention[idx, :, vie_length:] = 0\n        decoder_padding_mask_self_attention[idx, vie_length:, :] = 0\n        decoder_cross_attention_mask[idx, :, eng_length:] = 0\n        decoder_cross_attention_mask[idx, vie_length:, :] = 0\n        \n    decoder_self_attention_mask = look_ahead_mask * decoder_padding_mask_self_attention\n    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, seq_len):\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        \n    def forward(self):\n        even_index = torch.arange(0, self.d_model, 2).float()\n        denominator = torch.pow(10000, even_index/self.d_model)\n        position = torch.arange(self.seq_len).reshape(self.seq_len, 1)\n        even_PE = torch.sin(position/denominator)\n        odd_PE = torch.cos(position/denominator)\n        PE = torch.stack([even_PE, odd_PE], dim=2)\n        PE = torch.flatten(PE, start_dim = 1, end_dim=2) # shape: seq_len x d_model\n        return PE\n    \nclass SentenceEmbedding(nn.Module):\n    def __init__(self, max_sequence_length, d_model, vocab_size, pad_token):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.max_sequence_length = max_sequence_length\n        self.embedding = nn.Embedding(self.vocab_size, d_model)\n        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n        self.dropout = nn.Dropout(0.1)\n        self.pad_token = pad_token\n    \n    def batch_padding(self, batch, for_target=False):\n        tokenized = []\n        if not for_target:\n            for sentence_num in range(len(batch)):\n                tokenized.append(torch.tensor(np.concatenate((batch[sentence_num], \n                                              [self.pad_token]*(self.max_sequence_length - \n                                                                len(batch[sentence_num]))))\n                                             ).long())\n        else:\n            for sentence_num in range(len(batch)):\n                tokenized.append(torch.tensor(np.concatenate((batch[sentence_num][1:],\n                                              [self.pad_token]*(self.max_sequence_length - \n                                                                len(batch[sentence_num]) + 1)))).long()[:self.max_sequence_length])\n                \n        tokenized = torch.stack(tokenized)\n        return tokenized.to(get_device())\n    \n    def forward(self, x):\n        x = self.batch_padding(x)\n        x = self.embedding(x)\n        pos = self.position_encoder().to(get_device())\n        x = self.dropout(x + pos)\n        return x\n    \nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, mask):\n        batch_size, sequence_length, d_model = x.size()\n        qkv = self.qkv_layer(x)\n        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3)\n        q, k, v = qkv.chunk(3, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask)\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n        out = self.linear_layer(values)\n        return out\n    \nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(drop_prob)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n        \nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadSelfAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(drop_prob)\n        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout2 = nn.Dropout(drop_prob)\n    \n    def forward(self, x, self_attention_mask):\n        residual_x = x.clone()\n        x = self.attention(x, mask=self_attention_mask)\n        x = self.dropout1(x)\n        x = self.norm1(x + residual_x)\n        residual_x = x.clone()\n        x = self.ffn(x)\n        x = self.dropout2(x)\n        x = self.norm2(x + residual_x)\n        return x\n\nclass SequentialEncoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, self_attention_mask = inputs\n        for module in self._modules.values():\n            x = module(x, self_attention_mask)\n        return x\n    \nclass Encoder(nn.Module):\n    def __init__(self, d_model, \n                 ffn_hidden, \n                 num_heads, \n                 drop_prob,\n                 num_layers,\n                 max_sequence_length,\n                 vocab_size,\n                 pad_token):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, vocab_size, pad_token)\n        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n    \n    def forward(self, x, self_attention_mask):\n        x = self.sentence_embedding(x)\n        x = self.layers(x, self_attention_mask)\n        return x\n    \nclass MultiHeadCrossAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.kv_layer = nn.Linear(d_model, 2 * d_model)\n        self.q_layer = nn.Linear(d_model, d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, y, mask):\n        batch_size, eng_length, d_model = x.size()\n        _, vie_length, _ = y.size()\n        kv = self.kv_layer(x)\n        q = self.q_layer(y)\n        kv = kv.reshape(batch_size, eng_length, self.num_heads, 2 * self.head_dim)\n        q = q.reshape(batch_size, vie_length, self.num_heads, self.head_dim)\n        kv = kv.permute(0, 2, 1, 3)\n        q = q.permute(0, 2, 1, 3)\n        k, v = kv.chunk(2, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask)\n        values = values.permute(0,2,1,3).reshape(batch_size, vie_length, d_model)\n        out = self.linear_layer(values)\n        return out\n    \nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = MultiHeadSelfAttention(d_model, num_heads)\n        self.layer_norm1 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(drop_prob)\n        \n        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model, num_heads)\n        self.layer_norm2 = nn.LayerNorm(d_model)\n        self.dropout2 = nn.Dropout(drop_prob)\n        \n        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, drop_prob)\n        self.layer_norm3 = nn.LayerNorm(d_model)\n        self.dropout3 = nn.Dropout(drop_prob)\n        \n    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n        _y = y.clone()\n        y = self.self_attention(y, mask=self_attention_mask)\n        y = self.dropout1(y)\n        y = self.layer_norm1(y + _y)\n        \n        _y = y.clone()\n        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n        y = self.dropout2(y)\n        y = self.layer_norm2(y + _y)\n        \n        _y = y.clone()\n        y = self.ffn(y)\n        y = self.dropout3(y)\n        y = self.layer_norm3(y + _y)\n        return y\n    \nclass SequentialDecoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, y, self_attention_mask, cross_attention_mask = inputs\n        for module in self._modules.values():\n            y = module(x, y, self_attention_mask, cross_attention_mask)\n        return y\n    \nclass Decoder(nn.Module):\n    def __init__(self,\n                d_model,\n                ffn_hidden,\n                num_heads,\n                drop_prob,\n                num_layers,\n                max_sequence_length,\n                vocab_size,\n                pad_token):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, vocab_size, pad_token)\n        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n    \n    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n        y = self.sentence_embedding(y)\n        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n        return y\n    \nclass Transformer(nn.Module):\n    def __init__(self, \n                d_model, \n                ffn_hidden, \n                num_heads, \n                drop_prob,\n                num_layers,\n                max_eng_length,\n                max_vie_length,\n                eng_vocab_size,\n                vie_vocab_size,\n                eng_pad_token,\n                vie_pad_token):\n        super().__init__()\n        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, \n                               num_layers, max_eng_length, eng_vocab_size, eng_pad_token)\n        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, \n                               num_layers, max_vie_length, vie_vocab_size, vie_pad_token)\n        self.linear = nn.Linear(d_model, vie_vocab_size)\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        \n    def forward(self,\n               x,\n               y,\n               encoder_self_attention_mask=None,\n               decoder_self_attention_mask=None,\n               decoder_cross_attention_mask=None):\n        x = self.encoder(x, encoder_self_attention_mask)\n        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask)\n        out = self.linear(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-21T17:41:08.694787Z","iopub.execute_input":"2024-07-21T17:41:08.695371Z","iopub.status.idle":"2024-07-21T17:41:08.773339Z","shell.execute_reply.started":"2024-07-21T17:41:08.695324Z","shell.execute_reply":"2024-07-21T17:41:08.771964Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"d_model = 384\nbatch_size = 80\nffn_hidden = d_model * 4\nnum_heads = 8\ndrop_prob = 0.1\nnum_layers = 4\n\neng_vocab_size=len(eng_tokenizer.get_vocab())\nvie_vocab_size=len(vie_tokenizer.get_vocab())\neng_pad_token=eng_tokenizer.pad_token_id\nvie_pad_token=vie_tokenizer.pad_token_id\nvie_end_token=vie_tokenizer.sep_token_id\nvie_bos_token=vie_tokenizer.cls_token_id\n\ntransformer = Transformer(d_model=d_model, \n                        ffn_hidden=ffn_hidden, \n                        num_heads=num_heads, \n                        drop_prob=drop_prob,\n                        num_layers=num_layers,\n                        max_eng_length=context_length_eng,\n                        max_vie_length=context_length_vie,\n                        eng_vocab_size=eng_vocab_size,\n                        vie_vocab_size=vie_vocab_size,\n                        eng_pad_token=eng_pad_token,\n                        vie_pad_token=vie_pad_token)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T18:22:02.266822Z","iopub.execute_input":"2024-07-21T18:22:02.267308Z","iopub.status.idle":"2024-07-21T18:22:03.149591Z","shell.execute_reply.started":"2024-07-21T18:22:02.267274Z","shell.execute_reply":"2024-07-21T18:22:03.148449Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# criterian = nn.CrossEntropyLoss(weight=class_weights, ignore_index=vie_tokenizer.pad_token_id,\n#                                 reduction='none', label_smoothing=0.1)\ncriterian = nn.CrossEntropyLoss(ignore_index=vie_tokenizer.pad_token_id,\n                                reduction='none', label_smoothing=0.1)\n# When computing the loss, we are ignoring cases when the label is the padding token\nfor params in transformer.parameters():\n    if params.dim() > 1:\n        nn.init.xavier_uniform_(params)\n\noptim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:19.949712Z","iopub.execute_input":"2024-07-21T03:59:19.950201Z","iopub.status.idle":"2024-07-21T03:59:21.450197Z","shell.execute_reply.started":"2024-07-21T03:59:19.950154Z","shell.execute_reply":"2024-07-21T03:59:21.448549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\ntransformer.train()\ntransformer.to(device)\ntotal_loss = 0\nnum_epochs = 65\nevaluations = [\"i am so hungry i could eat a horse\", \n               \"i am unemployed and in urgent need of a job\",\n               \"i can see some progress in this project\",\n               \"that will prove something\",\n               \"i wil write some articles about this\",\n               \"if they are hiring i will apply\",\n               \"thing needs to change at certain point\",\n               \"this is just a scratch\",\n               \"what is the question again?\"]\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch}\")\n    if epoch == 45:\n        print(\"reduce lr to 3e-5\")\n        optim = torch.optim.Adam(transformer.parameters(), lr=3e-5)\n    if epoch == 55:\n        print(\"reduce lr to 1e-5\")\n        optim = torch.optim.Adam(transformer.parameters(), lr=1e-5)\n    iterator = iter(train_loader)\n    for batch_num, batch in enumerate(iterator):\n        transformer.train()\n        eng_batch, vie_batch = zip(*[batch[i] for i in range(len(batch))])\n        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, vie_batch)\n        optim.zero_grad()\n        vie_predictions = transformer(eng_batch,\n                                     vie_batch,\n                                     encoder_self_attention_mask.to(device),\n                                     decoder_self_attention_mask.to(device),\n                                     decoder_cross_attention_mask.to(device))\n        labels = transformer.decoder.sentence_embedding.batch_padding(vie_batch, for_target=True)\n        \n        loss = criterian(\n            vie_predictions.view(-1, vie_vocab_size).to(device),\n            labels.view(-1).to(device)\n        ).to(device)\n        \n        valid_indicies = torch.where(labels.view(-1) == vie_pad_token, False, True)\n        loss = loss.sum() / valid_indicies.sum()\n        loss.backward()\n        optim.step()\n  \n        if batch_num % 1000 == 0:\n            print(f\"Iteration {batch_num} : {loss.item()}\")\n            transformer.eval()\n            vie = [[vie_bos_token]]\n            eval_text = random.choice(evaluations)\n            context = [eng_tokenizer.encode(eval_text)]\n            print(\"Evaluating: \", eval_text)\n\n            for word_counter in range(context_length_vie):\n                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = \\\n                                        create_masks(context, vie)\n                vie_predictions = transformer(context,\n                                             vie,\n                                             encoder_self_attention_mask.to(device),\n                                             decoder_self_attention_mask.to(device),\n                                             decoder_cross_attention_mask.to(device))\n                next_token_prob_distribution = vie_predictions[0][word_counter]\n                next_token_index = int(torch.argmax(next_token_prob_distribution))\n                if next_token_index == vie_end_token:\n                    break\n                vie = [ vie[0] + [next_token_index] ]\n\n            print(f\"Evaluation: {vie_tokenizer.decode(vie[0])}\")\n            print(\"-------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T03:59:21.452690Z","iopub.execute_input":"2024-07-21T03:59:21.453765Z","iopub.status.idle":"2024-07-21T04:04:29.158497Z","shell.execute_reply.started":"2024-07-21T03:59:21.453715Z","shell.execute_reply":"2024-07-21T04:04:29.156759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(transformer.state_dict(), \"myTranslator\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T04:04:29.159615Z","iopub.status.idle":"2024-07-21T04:04:29.160083Z","shell.execute_reply.started":"2024-07-21T04:04:29.159873Z","shell.execute_reply":"2024-07-21T04:04:29.159893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.load_state_dict(torch.load(\"/kaggle/input/mytranslator/pytorch/default/1/myTranslator\", map_location=torch.device('cpu')))\ntransformer.eval()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T18:22:12.973579Z","iopub.execute_input":"2024-07-21T18:22:12.974029Z","iopub.status.idle":"2024-07-21T18:22:13.368787Z","shell.execute_reply.started":"2024-07-21T18:22:12.973976Z","shell.execute_reply":"2024-07-21T18:22:13.367473Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"Transformer(\n  (encoder): Encoder(\n    (sentence_embedding): SentenceEmbedding(\n      (embedding): Embedding(30522, 384)\n      (position_encoder): PositionalEncoding()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (layers): SequentialEncoder(\n      (0): EncoderLayer(\n        (attention): MultiHeadSelfAttention(\n          (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (1): EncoderLayer(\n        (attention): MultiHeadSelfAttention(\n          (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (2): EncoderLayer(\n        (attention): MultiHeadSelfAttention(\n          (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n      (3): EncoderLayer(\n        (attention): MultiHeadSelfAttention(\n          (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Decoder(\n    (sentence_embedding): SentenceEmbedding(\n      (embedding): Embedding(64001, 384)\n      (position_encoder): PositionalEncoding()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (layers): SequentialDecoder(\n      (0): DecoderLayer(\n        (self_attention): MultiHeadSelfAttention(\n          (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (encoder_decoder_attention): MultiHeadCrossAttention(\n          (kv_layer): Linear(in_features=384, out_features=768, bias=True)\n          (q_layer): Linear(in_features=384, out_features=384, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (layer_norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (1): DecoderLayer(\n        (self_attention): MultiHeadSelfAttention(\n          (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (encoder_decoder_attention): MultiHeadCrossAttention(\n          (kv_layer): Linear(in_features=384, out_features=768, bias=True)\n          (q_layer): Linear(in_features=384, out_features=384, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (layer_norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (2): DecoderLayer(\n        (self_attention): MultiHeadSelfAttention(\n          (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (encoder_decoder_attention): MultiHeadCrossAttention(\n          (kv_layer): Linear(in_features=384, out_features=768, bias=True)\n          (q_layer): Linear(in_features=384, out_features=384, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (layer_norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n      (3): DecoderLayer(\n        (self_attention): MultiHeadSelfAttention(\n          (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (encoder_decoder_attention): MultiHeadCrossAttention(\n          (kv_layer): Linear(in_features=384, out_features=768, bias=True)\n          (q_layer): Linear(in_features=384, out_features=384, bias=True)\n          (linear_layer): Linear(in_features=384, out_features=384, bias=True)\n        )\n        (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (ffn): PositionwiseFeedForward(\n          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n          (relu): ReLU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (layer_norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (linear): Linear(in_features=384, out_features=64001, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def translate(eng_sentence):\n    vie = [[vie_bos_token]]\n    print(\"English sentence:\")\n    print(eng_sentence)\n    context = [eng_tokenizer.encode(eng_sentence)]\n    print(\"----------------------------------------------------------------------------\")\n    for word_counter in range(context_length_vie):\n        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = \\\n                                create_masks(context, vie)\n        vie_predictions = transformer(context,\n                                     vie,\n                                     encoder_self_attention_mask.to(device),\n                                     decoder_self_attention_mask.to(device),\n                                     decoder_cross_attention_mask.to(device))\n        next_token_prob_distribution = vie_predictions[0][word_counter]\n        next_token_index = int(torch.argmax(next_token_prob_distribution))\n        if next_token_index == vie_end_token:\n            break\n        vie = [ vie[0] + [next_token_index] ]\n        \n    print(\"Vietnamese translation:\")\n    print(vie_tokenizer.decode(vie[0]))","metadata":{"execution":{"iopub.status.busy":"2024-07-21T18:22:43.988171Z","iopub.execute_input":"2024-07-21T18:22:43.988599Z","iopub.status.idle":"2024-07-21T18:22:43.998381Z","shell.execute_reply.started":"2024-07-21T18:22:43.988567Z","shell.execute_reply":"2024-07-21T18:22:43.997002Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# translated - will you do it for me\ntranslate(\"will you do it for me?\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T18:22:51.065945Z","iopub.execute_input":"2024-07-21T18:22:51.066380Z","iopub.status.idle":"2024-07-21T18:22:51.919366Z","shell.execute_reply.started":"2024-07-21T18:22:51.066349Z","shell.execute_reply":"2024-07-21T18:22:51.917630Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"English sentence:\nwill you do it for me?\n----------------------------------------------------------------------------\nVietnamese translation:\n<s> anh có làm điều đó cho tôi không\n","output_type":"stream"}]},{"cell_type":"code","source":"# translated - some random suprised things\ntranslate(\"just some random things\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T18:23:06.343320Z","iopub.execute_input":"2024-07-21T18:23:06.343733Z","iopub.status.idle":"2024-07-21T18:23:06.889125Z","shell.execute_reply.started":"2024-07-21T18:23:06.343702Z","shell.execute_reply":"2024-07-21T18:23:06.887754Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"English sentence:\njust some random things\n----------------------------------------------------------------------------\nVietnamese translation:\n<s> một vài điều bất ngờ\n","output_type":"stream"}]},{"cell_type":"code","source":"# translated - i hope this helps some of you\ntranslate(\"i hope this work helps some of you\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T18:23:37.263600Z","iopub.execute_input":"2024-07-21T18:23:37.264176Z","iopub.status.idle":"2024-07-21T18:23:39.065473Z","shell.execute_reply.started":"2024-07-21T18:23:37.264132Z","shell.execute_reply":"2024-07-21T18:23:39.063974Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"English sentence:\ni hope this works help some of you\n----------------------------------------------------------------------------\nVietnamese translation:\n<s> tôi hi vọng điều này làm việc một vài người trong số các bạn\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}